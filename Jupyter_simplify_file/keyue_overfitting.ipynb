{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 单个文件overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/p2m/lib/python3.7/site-packages/ipykernel_launcher.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "from models import backbones\n",
    "from train import train\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import io\n",
    "import numpy as np\n",
    "import torch\n",
    "from skimage import io, transform\n",
    "from models.p2m import P2MModel\n",
    "from utils.mesh import Ellipsoid\n",
    "from torchvision.transforms import Normalize\n",
    "from models.losses.p2m import P2MLoss\n",
    "from easydict import EasyDict as edict\n",
    "from utils.visualization import visualize_occupancy, visualize_pointcloud, visualize_mesh\n",
    "\n",
    "\n",
    "config  = {\n",
    "    \"experiment_name\": \"template\",\n",
    "    \"device\": \"cuda\",\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"batch_size\": 8,\n",
    "    \"max_epochs\": 100,\n",
    "    \"validate_every_n\": 50,\n",
    "    \"print_every_n\": 10,\n",
    "    \"loss\": edict({\"weights\": {\"normal\": 1.6e-4,\n",
    "        \"edge\": 0.3,\n",
    "        \"laplace\": 0.5,\n",
    "        \"move\": 0.1,\n",
    "        \"constant\": 1.,\n",
    "        \"chamfer\": [1., 1., 1.],\n",
    "        \"chamfer_opposite\": 1.,\n",
    "        \"reconst\": 0.,} \n",
    "    }),\n",
    "}\n",
    "\n",
    "# data = pickle.load(open(Path(\"/root/autodl-tmp/Pixel2Mesh_original-pytorch/datasets/data/shapenet/data_tf/04256520/1a4a8592046253ab5ff61a3a2a0e2484/rendering/01.dat\"), 'rb'), encoding=\"latin1\")\n",
    "# data = pickle.load(open(Path(\"/root/autodl-tmp/Pixel2Mesh_original-pytorch/datasets/data/shapenet/data_tf/02691156/1a04e3eab45ca15dd86060f189eb133/rendering/00.dat\"), 'rb'), encoding=\"latin1\")\n",
    "data = pickle.load(open(Path(\"/root/autodl-tmp/Pixel2Mesh_original-pytorch/datasets/data/shapenet/data_tf/02828884/1ac6a3d5c76c8b96edccc47bf0dcf5d3/rendering/01.dat\"), 'rb'), encoding=\"latin1\")\n",
    "pts, normals = data[:, :3], data[:, 3:]\n",
    "\n",
    "img = io.imread(\"/root/autodl-tmp/Pixel2Mesh_original-pytorch/datasets/data/shapenet/data_tf/02828884/1ac6a3d5c76c8b96edccc47bf0dcf5d3/rendering/01.png\")\n",
    "img[np.where(img[:, :, 3] == 0)] = 255\n",
    "img = transform.resize(img, (224, 224))\n",
    "img = img[:, :, :3].astype(np.float32)\n",
    "\n",
    "pts -= np.array([0., 0., -0.8])\n",
    "assert pts.shape[0] == normals.shape[0]\n",
    "length = pts.shape[0]\n",
    "\n",
    "img = torch.from_numpy(np.transpose(img, (2, 0, 1)))\n",
    "normalize_img = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "img_normalized = normalize_img(img)\n",
    "\n",
    "targets = {\n",
    "    \"images\": torch.tensor(img_normalized).unsqueeze(0).to(config[\"device\"]),\n",
    "    \"points\": torch.tensor(pts).unsqueeze(0).to(config[\"device\"]),\n",
    "    \"normals\": torch.tensor(normals).unsqueeze(0).to(config[\"device\"]),\n",
    "}\n",
    "\n",
    "options = {\n",
    "    \"hidden_dim\": 192,\n",
    "    \"last_hidden_dim\": 192,\n",
    "    \"coord_dim\": 3,\n",
    "    \"backbone\": \"vgg16\",\n",
    "    \"gconv_activation\": True,\n",
    "    \"z_threshold\": 0,\n",
    "    \"align_with_tensorflow\": False,\n",
    "    \"optim\": edict({\"weights\": {\"normal\": 1.6e-4,\n",
    "        \"edge\": 0.3,\n",
    "        \"laplace\": 0.5,\n",
    "        \"move\": 0.1,\n",
    "        \"constant\": 1.,\n",
    "        \"chamfer\": [1., 1., 1.],\n",
    "        \"chamfer_opposite\": 1.,\n",
    "        \"reconst\": 0.,} \n",
    "    })\n",
    "}\n",
    "\n",
    "options = edict(options)\n",
    "\n",
    "model = P2MModel(options, Ellipsoid([0., 0., -0.8]), [248., 248.], [111.5, 111.5], [0., 0., -0.8])\n",
    "model.to(config[\"device\"])\n",
    "\n",
    "loss_criterion = P2MLoss(config[\"loss\"], Ellipsoid([[0., 0., -0.8]]))\n",
    "\n",
    "loss_criterion.to(config[\"device\"])\n",
    "\n",
    "# optimizer = torch.optim.Adadelta(model.parameters(), config[\"learning_rate\"])\n",
    "optimizer = torch.optim.Adam(\n",
    "                params=list(model.parameters()),\n",
    "                lr=0.0001,\n",
    "                betas=(0.9, 0.999),\n",
    "                weight_decay=1.0e-06\n",
    "            )\n",
    "\n",
    "\n",
    "# for i in range(5001):\n",
    "\n",
    "#     optimizer.zero_grad()\n",
    "            \n",
    "#     preds = model(targets[\"images\"])\n",
    "\n",
    "#     loss, _ = loss_criterion(preds, targets)\n",
    "#     if i % 10 ==0:\n",
    "#         print(f\"{i}: {loss}\")\n",
    "        \n",
    "#     if i == 1000:\n",
    "#         preds1000 = preds\n",
    "#     if i == 2000:\n",
    "#         preds2000 = preds\n",
    "#     if i == 3000:\n",
    "#         preds3000 = preds\n",
    "#     if i == 4000:\n",
    "#         preds4000 = preds\n",
    "#     if i == 5000:\n",
    "#         preds5000 = preds\n",
    "        \n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在model基础上继续训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "                params=list(model.parameters()),\n",
    "                lr=0.0001,\n",
    "                betas=(0.9, 0.999),\n",
    "                weight_decay=1.0e-06\n",
    "            )\n",
    "\n",
    "\n",
    "for i in range(1000):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "            \n",
    "    preds = model(targets[\"images\"])\n",
    "\n",
    "    loss, _ = loss_criterion(preds, targets)\n",
    "    if i % 20 == 0:\n",
    "        print(f\"{i}: {loss}\")\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 人脸overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/p2m/lib/python3.7/site-packages/skimage/io/manage_plugins.py:23: UserWarning: Your installed pillow version is < 7.1.0. Several security issues (CVE-2020-11538, CVE-2020-10379, CVE-2020-10994, CVE-2020-10177) have been fixed in pillow 7.1.0 or higher. We recommend to upgrade this library.\n",
      "  from .collection import imread_collection_wrapper\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.float32'>\n",
      "<class 'numpy.float32'>\n",
      "(224, 224, 3)\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/p2m/lib/python3.7/site-packages/ipykernel_launcher.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "THCudaCheck FAIL file=/pytorch/aten/src/THC/THCGeneral.cpp line=383 error=11 : invalid argument\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 3.6249654293060303\n",
      "10: 0.4028460681438446\n",
      "20: 0.07092326134443283\n",
      "30: 0.03330230340361595\n",
      "40: 0.02197539247572422\n",
      "50: 0.0203960370272398\n",
      "60: 0.02533606067299843\n",
      "70: 0.019944583997130394\n",
      "80: 0.020622901618480682\n",
      "90: 0.028510551899671555\n",
      "100: 0.02212553471326828\n",
      "110: 0.028465306386351585\n",
      "120: 0.040453892201185226\n",
      "130: 0.02696302719414234\n",
      "140: 0.0313575342297554\n",
      "150: 0.02279849536716938\n",
      "160: 0.02222629450261593\n",
      "170: 0.028061451390385628\n",
      "180: 0.03771223872900009\n",
      "190: 0.026495633646845818\n",
      "200: 0.018998553976416588\n",
      "210: 0.027656907215714455\n",
      "220: 0.015310589224100113\n",
      "230: 0.01436105277389288\n",
      "240: 0.012713453732430935\n",
      "250: 0.011491440236568451\n",
      "260: 0.010485275648534298\n",
      "270: 0.010055535472929478\n",
      "280: 0.009545148350298405\n",
      "290: 0.00928723718971014\n",
      "300: 0.009105615317821503\n",
      "310: 0.00886804424226284\n",
      "320: 0.008772256784141064\n",
      "330: 0.008673641830682755\n",
      "340: 0.008521548472344875\n",
      "350: 0.00827530212700367\n",
      "360: 0.008186353370547295\n",
      "370: 0.00822235457599163\n",
      "380: 0.008016473613679409\n",
      "390: 0.008094299584627151\n",
      "400: 0.007756351493299007\n",
      "410: 0.007853333838284016\n",
      "420: 0.007504125125706196\n",
      "430: 0.007346161175519228\n",
      "440: 0.007269821595400572\n",
      "450: 0.0069875773042440414\n",
      "460: 0.006904192268848419\n",
      "470: 0.006703441496938467\n",
      "480: 0.006763223558664322\n",
      "490: 0.006908239331096411\n",
      "500: 0.006376541219651699\n",
      "510: 0.006328543182462454\n",
      "520: 0.006105450447648764\n",
      "530: 0.006625798996537924\n",
      "540: 0.006124608218669891\n",
      "550: 0.0060315728187561035\n",
      "560: 0.0058376784436404705\n",
      "570: 0.005625677295029163\n",
      "580: 0.005571630317717791\n",
      "590: 0.005460930522531271\n",
      "600: 0.005393916741013527\n",
      "610: 0.00515029625967145\n",
      "620: 0.005310909356921911\n",
      "630: 0.004969807341694832\n",
      "640: 0.004854120779782534\n",
      "650: 0.004706886131316423\n",
      "660: 0.004550641402602196\n",
      "670: 0.004416737239807844\n",
      "680: 0.0041771456599235535\n",
      "690: 0.003981227520853281\n",
      "700: 0.003890839172527194\n",
      "710: 0.0038699316792190075\n",
      "720: 0.003770835930481553\n",
      "730: 0.004036219324916601\n",
      "740: 0.004240051843225956\n",
      "750: 0.004930433817207813\n",
      "760: 0.004177164752036333\n",
      "770: 0.004428454674780369\n",
      "780: 0.004258411005139351\n",
      "790: 0.004126870539039373\n",
      "800: 0.004254362545907497\n",
      "810: 0.003782895626500249\n",
      "820: 0.0036234278231859207\n",
      "830: 0.003869331441819668\n",
      "840: 0.0036799819208681583\n",
      "850: 0.0037795561365783215\n",
      "860: 0.0034657574724406004\n",
      "870: 0.0038926354609429836\n",
      "880: 0.0077863698825240135\n",
      "890: 0.006330905947834253\n",
      "900: 0.005679972004145384\n",
      "910: 0.00431886175647378\n",
      "920: 0.004201899748295546\n",
      "930: 0.0039305733516812325\n",
      "940: 0.0037698799278587103\n",
      "950: 0.0036976647097617388\n",
      "960: 0.003575613023713231\n",
      "970: 0.003495743963867426\n",
      "980: 0.0034575951285660267\n",
      "990: 0.0034223173279315233\n"
     ]
    }
   ],
   "source": [
    "from models import backbones\n",
    "from train import train\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import io\n",
    "import numpy as np\n",
    "import torch\n",
    "from skimage import io, transform\n",
    "from models.p2m import P2MModel\n",
    "from utils.mesh import Ellipsoid\n",
    "from torchvision.transforms import Normalize\n",
    "from models.losses.p2m import P2MLoss\n",
    "from easydict import EasyDict as edict\n",
    "from utils.visualization import visualize_occupancy, visualize_pointcloud, visualize_mesh\n",
    "\n",
    "\n",
    "config  = {\n",
    "    \"experiment_name\": \"template\",\n",
    "    \"device\": \"cuda\",\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"batch_size\": 8,\n",
    "    \"max_epochs\": 100,\n",
    "    \"validate_every_n\": 50,\n",
    "    \"print_every_n\": 10,\n",
    "    \"loss\": edict({\"weights\": {\"normal\": 1.6e-4,\n",
    "        \"edge\": 0.3,\n",
    "        \"laplace\": 0.5,\n",
    "        \"move\": 0.1,\n",
    "        \"constant\": 1.,\n",
    "        \"chamfer\": [1., 1., 1.],\n",
    "        \"chamfer_opposite\": 1.,\n",
    "        \"reconst\": 0.,} \n",
    "    }),\n",
    "}\n",
    "\n",
    "\n",
    "data = np.loadtxt(\"/root/autodl-tmp/Pixel2Mesh_original-pytorch/datasets/data/AFLW2000-3D/AFLW2000/image00002_1.txt\")\n",
    "pts, normals = data[:, :3].astype(np.float32), data[:, 3:].astype(np.float32)\n",
    "\n",
    "length = pts.shape[0]\n",
    "choices = np.resize(np.random.permutation(length), 3000)\n",
    "pts, normals = pts[choices], normals[choices]\n",
    "\n",
    "pts = pts/1000.\n",
    "\n",
    "img = io.imread(\"/root/autodl-tmp/Pixel2Mesh_original-pytorch/datasets/data/AFLW2000-3D/AFLW2000/image00002.jpg\")\n",
    "img = transform.resize(img, (224, 224))\n",
    "img = img[:, :, :3].astype(np.float32)\n",
    "\n",
    "pts -= np.array([0., 0., -0.8])\n",
    "assert pts.shape[0] == normals.shape[0]\n",
    "length = pts.shape[0]\n",
    "\n",
    "img = torch.from_numpy(np.transpose(img, (2, 0, 1)))\n",
    "normalize_img = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "img_normalized = normalize_img(img)\n",
    "\n",
    "targets = {\n",
    "    \"images\": torch.tensor(img_normalized).unsqueeze(0).to(config[\"device\"]),\n",
    "    \"points\": torch.tensor(pts).unsqueeze(0).to(config[\"device\"]),\n",
    "    \"normals\": torch.tensor(normals).unsqueeze(0).to(config[\"device\"]),\n",
    "}\n",
    "\n",
    "options = {\n",
    "    \"hidden_dim\": 192,\n",
    "    \"last_hidden_dim\": 192,\n",
    "    \"coord_dim\": 3,\n",
    "    \"backbone\": \"vgg16\",\n",
    "    \"gconv_activation\": True,\n",
    "    \"z_threshold\": 0,\n",
    "    \"align_with_tensorflow\": False,\n",
    "    \"optim\": edict({\"weights\": {\"normal\": 1.6e-4,\n",
    "        \"edge\": 0.3,\n",
    "        \"laplace\": 0.5,\n",
    "        \"move\": 0.1,\n",
    "        \"constant\": 1.,\n",
    "        \"chamfer\": [1., 1., 1.],\n",
    "        \"chamfer_opposite\": 1.,\n",
    "        \"reconst\": 0.,} \n",
    "    })\n",
    "}\n",
    "\n",
    "options = edict(options)\n",
    "\n",
    "model = P2MModel(options, Ellipsoid([0., 0., -0.8]), [248., 248.], [111.5, 111.5], [0., 0., -0.8])\n",
    "model.to(config[\"device\"])\n",
    "\n",
    "loss_criterion = P2MLoss(config[\"loss\"], Ellipsoid([[0., 0., -0.8]]))\n",
    "\n",
    "loss_criterion.to(config[\"device\"])\n",
    "\n",
    "# optimizer = torch.optim.Adadelta(model.parameters(), config[\"learning_rate\"])\n",
    "optimizer = torch.optim.Adam(\n",
    "                params=list(model.parameters()),\n",
    "                lr=0.0001,\n",
    "                betas=(0.9, 0.999),\n",
    "                weight_decay=1.0e-06\n",
    "            )\n",
    "\n",
    "\n",
    "for i in range(1000):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "            \n",
    "    preds = model(targets[\"images\"])\n",
    "\n",
    "    loss, _ = loss_criterion(preds, targets)\n",
    "    if i % 10 ==0:\n",
    "        print(f\"{i}: {loss}\")\n",
    "        \n",
    "    if i == 1000:\n",
    "        preds1000 = preds\n",
    "    if i == 2000:\n",
    "        preds2000 = preds\n",
    "    if i == 3000:\n",
    "        preds3000 = preds\n",
    "    if i == 4000:\n",
    "        preds4000 = preds\n",
    "    if i == 5000:\n",
    "        preds5000 = preds\n",
    "        \n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b43e5002e346e6b5fb72b4dd8f8bf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eec6d90760f74b11b572e67303232925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "678bf42c49ea422481cc26fa5ebf1cde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_pointcloud(preds5000['pred_coord'][0].cpu().detach().numpy(), 10)\n",
    "visualize_pointcloud(preds5000['pred_coord'][1].cpu().detach().numpy(), 10)\n",
    "visualize_pointcloud(preds5000['pred_coord'][2].cpu().detach().numpy(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3169ac0b841a43f9aa1cac7af87ce2fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_pointcloud(pts, .01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p2m",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fc1be06a8457d4dbff30ef1da447cde0c16f1ca7f8c58cd09e5fb29b88c81a9e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
