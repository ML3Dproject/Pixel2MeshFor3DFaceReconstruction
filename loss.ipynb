{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.mesh import Ellipsoid\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from models.layers.chamfer_wrapper import ChamferDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import trimesh\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "a =  torch.Tensor([[2,14,11,4,-1,-1,-1,-1,1,4],[5,6,7,13,8,-1,-1,-1,2,5]])\n",
    "print(a.size(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "只是单纯实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a =  torch.Tensor([[2,14,11,4,-1,-1,-1,-1,1,4],[5,6,7,13,8,-1,-1,-1,2,5]])\n",
    "b = a[:, :-2]\n",
    "invalid_mask = b < 0\n",
    "all_valid_indices = b\n",
    "all_valid_indices[invalid_mask] = 0 \n",
    "print(all_valid_indices)\n",
    "inputs = np.array([[[2,3,4],[3,6,8],[-3,56,68],[23,-6,-8],[13,26,38],[43,-6,-8],[-3,-6,8],[3,-6,8],[-3,6,8],[-39,6,8],[53,6,8],[53,6,8],[63,6,28],[43,56,8],[43,56,28]]])\n",
    "\n",
    "vertices = inputs[:, all_valid_indices]\n",
    "print(vertices)\n",
    "\n",
    "vertices[:, invalid_mask] = 0\n",
    "neighbor_sum = torch.sum(vertices, 2)\n",
    "neighbor_count = lap_idx[:, -1].float()\n",
    "laplace = inputs - neighbor_sum / neighbor_count[None, :, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[122 176 188]\n",
      "  [123  94  84]]]\n",
      "[[[30.5 44.  47. ]\n",
      "  [24.6 18.8 16.8]]]\n"
     ]
    }
   ],
   "source": [
    "#example to test!\n",
    "b =  np.array([[2,14,11,4,-1,-1,-1,-1,1,4],[5,6,7,13,8,-1,-1,-1,2,5]])\n",
    "invalid_mask = b < 0\n",
    "all_valid_indices = b\n",
    "all_valid_indices[invalid_mask] = 0 \n",
    "# print(all_valid_indices)\n",
    "inputs = np.array([[[2,3,4],[3,6,8],[-3,56,68],[23,-6,-8],[13,26,38],[43,-6,-8],[-3,-6,8],[3,-6,8],[-3,6,8],[-39,6,8],[53,6,8],[53,6,8],[63,6,28],[43,56,8],[43,56,28]]])\n",
    "\n",
    "vertices = inputs[:, all_valid_indices]\n",
    "# print(vertices)\n",
    "\n",
    "vertices[:, invalid_mask] = 0\n",
    "neighbor_sum = np.sum(vertices, 2)\n",
    "neighbor_count = b[:, -1]\n",
    "print(neighbor_sum)\n",
    "laplace = neighbor_sum / neighbor_count[None, :, None]\n",
    "print(laplace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  1.,   3.,  46.],\n",
      "         [  2., 832., 336.]]])\n",
      "tensor([[  2., 832., 336.]])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.Tensor([[[3,53,6],[2,82,36]]])\n",
    "x2 = torch.Tensor([[[1,3,46],[2,832,336]]])\n",
    "x3 = torch.Tensor([[[3,553,65],[52,852,436]]])\n",
    "x = [x1,x2,x3]\n",
    "print(x[1])\n",
    "print(x[1][:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[0, 0, 0, 0]])]\n",
      "tensor([[[0.]]])\n"
     ]
    }
   ],
   "source": [
    "with open('datasets/data/semi-sphere/semi-sphere.dat', \"rb\") as fp:\n",
    "            fp_info = pickle.load(fp, encoding='latin1')\n",
    "# print(fp_info[0])# shape: n_pts * 3\n",
    "# print(fp_info[1][1][0])\n",
    "# print(fp_info)\n",
    "laplace_idx = []\n",
    "laplace_idx.append(torch.tensor(fp_info[7][0], dtype=torch.long))\n",
    "print(laplace_idx)#very important!! laplace_idx is in a [], if we want to get out the number, we need add [0] first\n",
    "neighbor_count = laplace_idx[0][:, -1].float()#-1 to represent None\n",
    "print(neighbor_count[None, :, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0,   0],\n",
      "        [  0,   1],\n",
      "        [  0,   3],\n",
      "        ...,\n",
      "        [140,  77],\n",
      "        [140, 138],\n",
      "        [140, 140]])\n",
      "tensor([  0,   1,   3,  69,  72,  73,   0,   1,   2,   3,   4,  70,  72,   1,\n",
      "          2,   4,   5,  70,  71,   0,   1,   3,   4,   6,   8,   9,  10,  73,\n",
      "          1,   2,   3,   4,   5,   7,   8,   2,   4,   5,   7,  11,  71, 124,\n",
      "          3,   6,   9,  13,  73,  74, 104, 106,   4,   5,   7,   8,  11,  14,\n",
      "         15,  16,   3,   4,   7,   8,  10,  12,  15,   3,   6,   9,  10,  13,\n",
      "          3,   8,   9,  10,  12,  13,  20,   5,   7,  11,  14,  17, 124,   8,\n",
      "         10,  12,  15,  20,  21,  29,   6,   9,  10,  13,  20, 106, 107,   7,\n",
      "         11,  14,  16,  17,  18,  19,  24,   7,   8,  12,  15,  16,  29,   7,\n",
      "         14,  15,  16,  18,  22,  26,  29,  32,  11,  14,  17,  24,  27, 124,\n",
      "        127,  14,  16,  18,  22,  14,  19,  24,  28,  10,  12,  13,  20,  21,\n",
      "         23,  25, 107,  12,  20,  21,  23,  29,  30,  31,  16,  18,  22,  26,\n",
      "         20,  21,  23,  25,  30, 111,  14,  17,  19,  24,  27,  28,  20,  23,\n",
      "         25, 107, 109, 110, 111, 112,  16,  22,  26,  32,  34,  17,  24,  27,\n",
      "         28,  33, 127, 131, 134,  19,  24,  27,  28,  33,  12,  15,  16,  21,\n",
      "         29,  31,  32,  21,  23,  30,  31,  37,  39, 111,  21,  29,  30,  31,\n",
      "         32,  38,  39,  16,  26,  29,  31,  32,  33,  34,  35,  38,  27,  28,\n",
      "         32,  33,  34,  35, 134,  26,  32,  33,  34,  32,  33,  35,  36,  38,\n",
      "        134,  35,  36,  38,  41,  45, 134,  30,  37,  39,  40,  42, 111, 116,\n",
      "         31,  32,  35,  36,  38,  39,  41,  30,  31,  37,  38,  39,  40,  41,\n",
      "         37,  39,  40,  41,  42,  44,  75,  36,  38,  39,  40,  41,  43,  44,\n",
      "         45,  37,  40,  42,  75, 116,  41,  43,  44,  45,  76, 140,  40,  41,\n",
      "         43,  44,  75,  76,  78,  36,  41,  43,  45, 134, 135, 138, 140,  46,\n",
      "         47,  48,  52,  46,  47,  48,  50,  51,  52,  54,  55,  56,  57,  59,\n",
      "         62,  63,  46,  47,  48,  49,  50,  48,  49,  50,  53,  47,  48,  49,\n",
      "         50,  53,  56,  58, 123,  47,  51,  52,  54,  46,  47,  51,  52,  49,\n",
      "         50,  53,  47,  51,  54,  55,  47,  54,  55,  57,  47,  50,  56,  58,\n",
      "         63,  68,  47,  55,  57,  59,  50,  56,  58,  68, 122, 123,  47,  57,\n",
      "         59,  60,  62,  59,  60,  61,  62,  60,  61,  62,  64,  65,  66,  69,\n",
      "         72,  47,  59,  60,  61,  62,  63,  72,  47,  56,  62,  63,  68,  70,\n",
      "         72,  61,  64,  65,  61,  64,  65,  66,  61,  65,  66,  67,  69,  66,\n",
      "         67,  69,  98,  56,  58,  63,  68,  70,  71, 122,   0,  61,  66,  67,\n",
      "         69,  72,  73,  74,  98,  99, 100,   1,   2,  63,  68,  70,  71,  72,\n",
      "          2,   5,  68,  70,  71, 122, 124,   0,   1,  61,  62,  63,  69,  70,\n",
      "         72,   0,   3,   6,  69,  73,  74,   6,  69,  73,  74,  99, 101, 102,\n",
      "        103, 104, 105,  40,  42,  44,  75,  78,  80,  83,  85, 116, 120, 121,\n",
      "         43,  44,  76,  77,  78,  79,  81, 140,  76,  77,  81, 137, 138, 140,\n",
      "         44,  75,  76,  78,  79,  82,  83,  84,  87,  76,  78,  79,  81,  82,\n",
      "         88,  89,  90,  91,  92,  75,  80,  85, 121,  76,  77,  79,  81,  88,\n",
      "         93,  94,  96,  97, 137, 139,  78,  79,  82,  84,  89,  75,  78,  83,\n",
      "         85,  86,  87,  78,  82,  84,  87,  75,  80,  83,  85,  86,  83,  85,\n",
      "         86,  78,  83,  84,  87,  79,  81,  88,  91,  95,  97,  79,  82,  89,\n",
      "         90,  79,  89,  90,  92,  79,  88,  91,  92,  95,  79,  90,  91,  92,\n",
      "         81,  93,  96,  97,  81,  94,  96, 139,  88,  91,  95,  97,  81,  93,\n",
      "         94,  96,  81,  88,  93,  95,  97,  67,  69,  98, 100,  69,  74,  99,\n",
      "        100, 101,  69,  98,  99, 100,  74,  99, 101, 102,  74, 101, 102, 103,\n",
      "         74, 102, 103, 105,   6,  74, 104, 105, 106,  74, 103, 104, 105,   6,\n",
      "         13, 104, 106, 107, 108,  13,  20,  25, 106, 107, 108, 109, 106, 107,\n",
      "        108, 109,  25, 107, 108, 109, 110,  25, 109, 110, 112,  23,  25,  30,\n",
      "         37, 111, 112, 113, 116,  25, 110, 111, 112, 113, 111, 112, 113, 114,\n",
      "        115, 116, 117, 113, 114, 115, 113, 114, 115, 117,  37,  42,  75, 111,\n",
      "        113, 116, 117, 118, 119, 120, 113, 115, 116, 117, 118, 116, 117, 118,\n",
      "        119, 116, 118, 119, 120,  75, 116, 119, 120, 121,  75,  80, 120, 121,\n",
      "         58,  68,  71, 122, 123, 124, 125, 126,  50,  58, 122, 123, 125,   5,\n",
      "         11,  17,  71, 122, 124, 126, 127, 128, 122, 123, 125, 126, 122, 124,\n",
      "        125, 126, 128, 129,  17,  27, 124, 127, 128, 131, 124, 126, 127, 128,\n",
      "        129, 130, 131, 132, 126, 128, 129, 130, 128, 129, 130, 132,  27, 127,\n",
      "        128, 131, 132, 133, 134, 135, 128, 130, 131, 132, 133, 131, 132, 133,\n",
      "        135, 136,  27,  33,  35,  36,  45, 131, 134, 135,  45, 131, 133, 134,\n",
      "        135, 136, 138, 133, 135, 136, 137, 138,  77,  81, 136, 137, 138, 139,\n",
      "         45,  77, 135, 136, 137, 138, 140,  81,  94, 137, 139,  43,  45,  76,\n",
      "         77, 138, 140])\n"
     ]
    }
   ],
   "source": [
    "edges = []\n",
    "edges.append(torch.tensor(fp_info[1][1][0], dtype=torch.long))\n",
    "print(edges[0])\n",
    "print(edges[0][:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 0]], dtype=torch.uint8)\n",
      "tensor([[0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "indices = laplace_idx[0]\n",
    "invalid_mask = indices < 0\n",
    "all_valid_indices = indices.clone()\n",
    "all_valid_indices[invalid_mask] = 0  # do this to avoid negative indices\n",
    "print(invalid_mask)\n",
    "print(all_valid_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0]\n",
      " [  0   1]\n",
      " [  0   3]\n",
      " ...\n",
      " [140  77]\n",
      " [140 138]\n",
      " [140 140]] [ 0.22327143 -0.22333778 -0.1934162  -0.1729967  -0.20677061 -0.24465429\n",
      " -0.22333778  0.22327143 -0.22333778 -0.17656402 -0.20387857 -0.20387857\n",
      " -0.18875488 -0.22333778  0.22327143 -0.22333778 -0.22333778 -0.22333778\n",
      " -0.22333778 -0.1934162  -0.17656402  0.22327143 -0.17656402 -0.16346652\n",
      " -0.17656402 -0.21624588 -0.17656402 -0.1934162  -0.20387857 -0.22333778\n",
      " -0.17656402  0.22327143 -0.20387857 -0.18875488 -0.20387857 -0.22333778\n",
      " -0.20387857  0.22327143 -0.18875488 -0.22333778 -0.20387857 -0.17656402\n",
      " -0.16346652  0.22327143 -0.23117657 -0.18875488 -0.20677061 -0.15411771\n",
      " -0.23117657 -0.20677061 -0.18875488 -0.18875488  0.22327143 -0.18875488\n",
      " -0.20677061 -0.17475306 -0.20677061 -0.16346652 -0.17656402 -0.20387857\n",
      " -0.18875488  0.22327143 -0.20387857 -0.20387857 -0.22333778 -0.21624588\n",
      " -0.23117657  0.22327143 -0.24969923 -0.24969923 -0.17656402 -0.20387857\n",
      " -0.24969923  0.22327143 -0.20387857 -0.20387857 -0.18875488 -0.22333778\n",
      " -0.20677061  0.22327143 -0.20677061 -0.22333778 -0.1934162  -0.20387857\n",
      " -0.20387857  0.22327143 -0.22333778 -0.18875488 -0.20387857 -0.20387857\n",
      " -0.18875488 -0.24969923 -0.20387857  0.22327143 -0.18875488 -0.22333778\n",
      " -0.20387857 -0.17475306 -0.20677061  0.22327143 -0.16346652 -0.18875488\n",
      " -0.26693971 -0.26693971 -0.20677061 -0.20677061 -0.22333778 -0.22333778\n",
      "  0.22327143 -0.1934162  -0.22333778 -0.16346652 -0.16346652 -0.1934162\n",
      "  0.22327143 -0.24969923 -0.24969923 -0.21624588 -0.17656402 -0.15290893\n",
      " -0.22333778 -0.18875488  0.22327143 -0.22333778 -0.18875488 -0.17656402\n",
      " -0.22333778 -0.26693971 -0.24969923  0.22327143 -0.40775714 -0.26693971\n",
      "  0.22327143 -0.31584732 -0.35312804 -0.18875488 -0.18875488 -0.18875488\n",
      "  0.22327143 -0.18875488 -0.20677061 -0.17475306 -0.18875488 -0.20387857\n",
      " -0.18875488  0.22327143 -0.22333778 -0.20387857 -0.20387857 -0.20387857\n",
      " -0.24969923 -0.40775714  0.22327143 -0.35312804 -0.20677061 -0.22333778\n",
      "  0.22327143 -0.20677061 -0.22333778 -0.20677061 -0.20677061 -0.22333778\n",
      " -0.31584732  0.22327143 -0.20677061 -0.27353181 -0.17475306 -0.20677061\n",
      "  0.22327143 -0.18875488 -0.23117657 -0.26693971 -0.17475306 -0.23117657\n",
      " -0.21624588 -0.35312804  0.22327143 -0.21624588 -0.35312804 -0.18875488\n",
      " -0.20677061  0.22327143 -0.23117657 -0.18875488 -0.20677061 -0.17475306\n",
      " -0.17475306 -0.35312804 -0.27353181 -0.23117657  0.22327143 -0.24969923\n",
      " -0.20387857 -0.22333778 -0.17656402 -0.20387857  0.22327143 -0.20387857\n",
      " -0.17656402 -0.20387857 -0.22333778  0.22327143 -0.20387857 -0.20387857\n",
      " -0.20387857 -0.18875488 -0.20387857 -0.20387857 -0.20387857  0.22327143\n",
      " -0.17656402 -0.20387857 -0.20387857 -0.15290893 -0.21624588 -0.17656402\n",
      " -0.17656402  0.22327143 -0.17656402 -0.24969923 -0.1934162  -0.17656402\n",
      " -0.18875488 -0.24969923 -0.17656402  0.22327143 -0.28832784 -0.22333778\n",
      " -0.18875488 -0.35312804 -0.24969923 -0.28832784  0.22327143 -0.1934162\n",
      " -0.22333778  0.22327143 -0.24465429 -0.22333778 -0.20677061 -0.24465429\n",
      "  0.22327143 -0.22333778 -0.20677061 -0.20677061 -0.20677061 -0.20387857\n",
      "  0.22327143 -0.20387857 -0.20387857 -0.24969923 -0.18875488 -0.16646616\n",
      " -0.20387857 -0.17656402 -0.22333778 -0.22333778  0.22327143 -0.20387857\n",
      " -0.18875488 -0.20387857 -0.20387857 -0.20387857 -0.20387857  0.22327143\n",
      " -0.20387857 -0.18875488 -0.20387857 -0.20387857  0.22327143 -0.18875488\n",
      " -0.24969923 -0.20387857 -0.15792366 -0.20677061 -0.18875488 -0.18875488\n",
      " -0.18875488  0.22327143 -0.20677061 -0.18875488 -0.17475306 -0.24969923\n",
      " -0.24969923  0.22327143 -0.1934162  -0.20387857 -0.20677061  0.22327143\n",
      " -0.22333778 -0.20677061 -0.20677061 -0.24465429 -0.20387857 -0.18875488\n",
      " -0.22333778  0.22327143 -0.15792366 -0.18875488 -0.17656402 -0.20677061\n",
      " -0.17475306 -0.20677061  0.22327143 -0.17475306 -0.18875488 -0.18875488\n",
      " -0.20677061  0.22327143 -0.20387857 -0.35312804 -0.40775714 -0.20387857\n",
      "  0.22327143 -0.17656402 -0.13346985 -0.20387857 -0.20387857 -0.20387857\n",
      " -0.20387857 -0.15792366 -0.20387857 -0.17656402 -0.14416392 -0.14416392\n",
      " -0.35312804 -0.17656402  0.22327143 -0.35312804 -0.23117657 -0.35312804\n",
      "  0.22327143 -0.26693971 -0.49939847 -0.13346985 -0.23117657 -0.26693971\n",
      "  0.22327143 -0.32693304 -0.20677061 -0.20677061 -0.23117657 -0.20387857\n",
      "  0.22327143 -0.40775714 -0.40775714 -0.40775714 -0.20387857 -0.40775714\n",
      "  0.22327143 -0.49939847 -0.32693304  0.22327143 -0.20387857 -0.40775714\n",
      "  0.22327143 -0.40775714 -0.20387857 -0.40775714  0.22327143 -0.40775714\n",
      " -0.15792366 -0.20677061  0.22327143 -0.24465429 -0.22333778 -0.22333778\n",
      " -0.20387857 -0.40775714  0.22327143 -0.35312804 -0.20677061 -0.24465429\n",
      "  0.22327143 -0.22333778 -0.20677061 -0.27353181 -0.17656402 -0.35312804\n",
      "  0.22327143 -0.35312804 -0.24969923 -0.35312804  0.22327143 -0.26693971\n",
      " -0.28832784 -0.26693971  0.22327143 -0.18875488 -0.32693304 -0.26693971\n",
      " -0.23117657 -0.1462089  -0.17475306 -0.14416392 -0.24969923 -0.28832784\n",
      " -0.18875488  0.22327143 -0.20387857 -0.18875488 -0.14416392 -0.22333778\n",
      " -0.20387857  0.22327143 -0.20387857 -0.20387857 -0.18875488 -0.32693304\n",
      "  0.22327143 -0.49939847 -0.26693971 -0.49939847  0.22327143 -0.35312804\n",
      " -0.23117657 -0.35312804  0.22327143 -0.35312804 -0.1934162  -0.35312804\n",
      "  0.22327143 -0.22333778 -0.40775714 -0.22333778 -0.22333778 -0.20387857\n",
      "  0.22327143 -0.20387857 -0.20387857 -0.18875488 -0.1729967  -0.1462089\n",
      " -0.1934162  -0.22333778  0.22327143 -0.1462089  -0.1729967  -0.12894413\n",
      " -0.22333778 -0.1934162  -0.22333778 -0.20387857 -0.22333778 -0.20387857\n",
      " -0.20387857  0.22327143 -0.20387857 -0.18875488 -0.22333778 -0.20387857\n",
      " -0.20387857 -0.20387857  0.22327143 -0.18875488 -0.17656402 -0.20677061\n",
      " -0.18875488 -0.17475306 -0.18875488 -0.18875488 -0.1462089  -0.18875488\n",
      "  0.22327143 -0.24465429 -0.1934162  -0.20677061 -0.1729967   0.22327143\n",
      " -0.18235454 -0.15411771 -0.12894413 -0.18235454  0.22327143 -0.20387857\n",
      " -0.2354187  -0.2354187  -0.2354187  -0.20387857 -0.2354187  -0.15792366\n",
      " -0.1934162  -0.15792366  0.22327143 -0.1367659  -0.22333778 -0.1729967\n",
      " -0.1934162  -0.12894413 -0.1934162  -0.22333778 -0.20677061 -0.18875488\n",
      "  0.22327143 -0.20677061 -0.16346652 -0.15411771 -0.1462089  -0.20677061\n",
      " -0.20677061  0.22327143 -0.1729967  -0.24465429 -0.22333778 -0.24465429\n",
      " -0.17656402 -0.1367659  -0.16346652  0.22327143 -0.14416392 -0.21624588\n",
      " -0.1934162  -0.24969923 -0.24969923 -0.15411771 -0.14416392  0.22327143\n",
      " -0.12894413 -0.20387857 -0.18235454 -0.2354187  -0.2354187  -0.20387857\n",
      " -0.2354187  -0.22333778  0.22327143 -0.35312804 -0.40775714 -0.1462089\n",
      " -0.1729967  -0.12894413  0.22327143 -0.1729967  -0.22333778 -0.22333778\n",
      " -0.22333778 -0.1934162  -0.1729967  -0.22333778 -0.21624588 -0.20387857\n",
      "  0.22327143 -0.35312804 -0.35312804 -0.1729967  -0.1934162   0.22327143\n",
      " -0.27353181 -0.38683239 -0.31584732 -0.24969923 -0.35312804  0.22327143\n",
      " -0.40775714 -0.1934162  -0.35312804 -0.27353181  0.22327143 -0.43249176\n",
      " -0.38683239 -0.43249176  0.22327143 -0.24969923 -0.31584732 -0.40775714\n",
      "  0.22327143 -0.18235454 -0.1729967   0.22327143 -0.27353181 -0.31584732\n",
      " -0.27353181 -0.2354187  -0.35312804  0.22327143 -0.40775714 -0.2354187\n",
      " -0.40775714  0.22327143 -0.40775714 -0.20387857 -0.27353181  0.22327143\n",
      " -0.35312804 -0.35312804 -0.2354187  -0.40775714 -0.35312804  0.22327143\n",
      " -0.22333778  0.22327143 -0.40775714 -0.35312804 -0.22333778  0.22327143\n",
      " -0.40775714 -0.40775714 -0.31584732 -0.35312804  0.22327143 -0.35312804\n",
      " -0.22333778 -0.40775714 -0.40775714  0.22327143 -0.1934162  -0.27353181\n",
      " -0.35312804 -0.35312804  0.22327143 -0.40775714 -0.22333778  0.22327143\n",
      " -0.40775714 -0.1934162  -0.20387857  0.22327143 -0.35312804 -0.35312804\n",
      " -0.22333778 -0.40775714 -0.35312804  0.22327143 -0.2354187  -0.35312804\n",
      "  0.22327143 -0.40775714 -0.2354187  -0.40775714  0.22327143 -0.40775714\n",
      " -0.2354187  -0.40775714  0.22327143 -0.40775714 -0.23117657 -0.20387857\n",
      "  0.22327143 -0.35312804 -0.27353181 -0.2354187  -0.40775714 -0.35312804\n",
      "  0.22327143 -0.20677061 -0.22333778 -0.27353181  0.22327143 -0.22333778\n",
      " -0.31584732 -0.20387857 -0.18875488 -0.18875488 -0.22333778  0.22327143\n",
      " -0.28832784 -0.24969923 -0.31584732 -0.28832784  0.22327143 -0.35312804\n",
      " -0.23117657 -0.24969923 -0.35312804  0.22327143 -0.35312804 -0.26693971\n",
      " -0.35312804  0.22327143 -0.35312804 -0.20677061 -0.17475306 -0.18875488\n",
      " -0.18875488  0.22327143 -0.23117657 -0.18875488 -0.15411771 -0.23117657\n",
      " -0.35312804 -0.23117657  0.22327143 -0.24969923 -0.18875488 -0.24969923\n",
      "  0.22327143 -0.35312804 -0.28832784 -0.16646616 -0.24969923 -0.35312804\n",
      "  0.22327143 -0.49939847 -0.28832784 -0.49939847  0.22327143 -0.35312804\n",
      " -0.16646616 -0.20387857 -0.12894413 -0.15411771 -0.16646616  0.22327143\n",
      " -0.20387857 -0.2354187  -0.2354187  -0.20387857 -0.24969923 -0.35312804\n",
      " -0.20387857  0.22327143 -0.35312804 -0.2354187  -0.35312804  0.22327143\n",
      " -0.40775714 -0.2354187  -0.40775714  0.22327143 -0.35312804 -0.1934162\n",
      " -0.20387857 -0.35312804  0.22327143 -0.35312804 -0.22333778 -0.40775714\n",
      " -0.35312804  0.22327143 -0.20677061 -0.18875488 -0.18875488  0.22327143\n",
      " -0.23117657 -0.16346652 -0.26693971 -0.20677061 -0.23117657 -0.27353181\n",
      " -0.23117657  0.22327143 -0.35312804 -0.17656402 -0.1934162  -0.17656402\n",
      " -0.17656402 -0.16346652  0.22327143 -0.1934162  -0.1934162  -0.16346652\n",
      " -0.26693971 -0.35312804  0.22327143 -0.31584732 -0.20677061 -0.1934162\n",
      " -0.31584732  0.22327143 -0.20677061 -0.31584732 -0.22333778 -0.20677061\n",
      " -0.1934162   0.22327143 -0.20677061 -0.20677061 -0.16346652 -0.20677061\n",
      " -0.20677061  0.22327143 -0.26693971 -0.26693971 -0.17475306 -0.23117657\n",
      " -0.31584732 -0.26693971  0.22327143 -0.40775714 -0.26693971 -0.40775714\n",
      "  0.22327143 -0.35312804 -0.17475306 -0.20677061 -0.17475306  0.22327143\n",
      " -0.23117657 -0.23117657 -0.17475306 -0.18875488 -0.23117657 -0.35312804\n",
      " -0.23117657  0.22327143 -0.30581786 -0.23117657 -0.30581786  0.22327143\n",
      " -0.24969923 -0.30581786 -0.17475306 -0.18875488 -0.20677061 -0.20677061\n",
      " -0.17475306 -0.17475306  0.22327143 -0.18875488 -0.18875488 -0.18875488\n",
      " -0.24969923 -0.18875488  0.22327143 -0.24969923 -0.20387857 -0.30581786\n",
      " -0.24969923  0.22327143 -0.27353181 -0.24969923 -0.24465429 -0.1729967\n",
      " -0.27353181  0.22327143 -0.22333778 -0.31584732 -0.18875488 -0.22333778\n",
      " -0.20387857 -0.24969923 -0.22333778  0.22327143 -0.22333778 -0.22333778\n",
      " -0.40775714 -0.31584732  0.22327143 -0.24465429 -0.20677061 -0.20677061\n",
      " -0.24465429 -0.22333778  0.22327143] (141, 141)\n"
     ]
    }
   ],
   "source": [
    "print(*fp_info[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't use starred expression here (2067671154.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_3099/2067671154.py\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m can't use starred expression here\n"
     ]
    }
   ],
   "source": [
    "# indices, value, size = *fp_info[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  0,   0],\n",
      "        [  0,   1],\n",
      "        [  0,   2],\n",
      "        ...,\n",
      "        [155, 153],\n",
      "        [155, 154],\n",
      "        [155, 155]]), tensor([[  0,   0],\n",
      "        [  0, 156],\n",
      "        [  0, 157],\n",
      "        ...,\n",
      "        [617, 612],\n",
      "        [617, 616],\n",
      "        [617, 617]]), tensor([[   0,    0],\n",
      "        [   0,  618],\n",
      "        [   0,  619],\n",
      "        ...,\n",
      "        [2465, 2463],\n",
      "        [2465, 2464],\n",
      "        [2465, 2465]])]\n",
      "################\n",
      "tensor([[156, 157, 168,  ...,  -1,   0,   7],\n",
      "        [156, 158, 159,  ...,  -1,   1,   7],\n",
      "        [157, 158, 162,  ...,  -1,   2,   5],\n",
      "        ...,\n",
      "        [140, 154, 550,  ...,  -1, 615,   6],\n",
      "        [142, 154, 557,  ...,  -1, 616,   6],\n",
      "        [142, 155, 559,  ...,  -1, 617,   6]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mesh_pos = [0., 0., -0.8]\n",
    "ellipsoid = Ellipsoid(mesh_pos)\n",
    "\n",
    "pd.set_option('display.max_columns',1000)\n",
    "pd.set_option('max_row',300)\n",
    "pd.set_option('display.float_format',lambda x:'%.5f' % x)\n",
    "\n",
    "print(ellipsoid.edges)\n",
    "print('################')\n",
    "print(ellipsoid.laplace_idx[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'f' b'1' b'2' b'3']\n",
      " [b'f' b'2' b'4' b'5']\n",
      " [b'f' b'3' b'2' b'5']\n",
      " ...\n",
      " [b'f' b'143' b'141' b'155']\n",
      " [b'f' b'156' b'143' b'155']\n",
      " [b'f' b'143' b'156' b'137']]\n",
      "[[b'f' b'1' b'2' b'3']\n",
      " [b'f' b'2' b'4' b'5']\n",
      " [b'f' b'3' b'2' b'5']\n",
      " ...\n",
      " [b'f' b'143' b'141' b'155']\n",
      " [b'f' b'156' b'143' b'155']\n",
      " [b'f' b'143' b'156' b'137']]\n",
      "[[b'f' b'1' b'2' b'3']\n",
      " [b'f' b'2' b'4' b'5']\n",
      " [b'f' b'3' b'2' b'5']\n",
      " ...\n",
      " [b'f' b'143' b'141' b'155']\n",
      " [b'f' b'156' b'143' b'155']\n",
      " [b'f' b'143' b'156' b'137']]\n",
      "[[b'f' b'1' b'2' b'3']\n",
      " [b'f' b'2' b'4' b'5']\n",
      " [b'f' b'3' b'2' b'5']\n",
      " ...\n",
      " [b'f' b'143' b'141' b'155']\n",
      " [b'f' b'156' b'143' b'155']\n",
      " [b'f' b'143' b'156' b'137']]\n"
     ]
    }
   ],
   "source": [
    "faces = []\n",
    "obj_fmt_faces = []#usage??\n",
    "        # faces: f * 3, original ellipsoid, and two after deformations\n",
    "for i in range(1, 5):\n",
    "    # face_file = os.path.join('datasets/data/ellipsoid/', \"face%d.obj\" % i)\n",
    "    face_file = os.path.join('datasets/data/semi-sphere/', \"semi%d.obj\" % i)\n",
    "    faces = np.genfromtxt(face_file, dtype='|S32')\n",
    "    obj_fmt_faces.append(faces)\n",
    "    print(obj_fmt_faces[0])\n",
    "#     faces.append(torch.tensor(faces[:, 1:].astype(np.int) - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "tensor([ 3., 53.,  6.])\n",
      "##############\n",
      "tensor([1, 0])\n",
      "tensor([53.,  3.])\n",
      "tensor([ 2., 82., 36.])\n",
      "##############\n",
      "tensor([2, 0])\n",
      "tensor([36.,  2.])\n",
      "tensor([[53.,  3.],\n",
      "        [36.,  2.]])\n"
     ]
    }
   ],
   "source": [
    "gt_normal = torch.Tensor([[3,53,6],[2,82,36]])\n",
    "print(gt_normal.shape)\n",
    "indices = torch.Tensor([[1,0],[2,0]])\n",
    "for t, i in zip(gt_normal, indices.long()):\n",
    "    print(t)\n",
    "    print('##############')\n",
    "    print(i)\n",
    "    print(t[i])\n",
    "nearest_normals = torch.stack([t[i] for t, i in zip(gt_normal, indices.long())])#add one more []\n",
    "print(nearest_normals)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3dproject",
   "language": "python",
   "name": "3dproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15 (default, Nov 24 2022, 21:12:53) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
